'''
AHAAB predict_ahaab submodule
Part of the AHAAB train module

ahaab/src/
└──predict
    └──predict_ahaab.py

Submodule list:
    
    === predict_ahaab_atom ===
'''
# AHAAB imports
from tools import formats
from tools.utils import standardize_feature_data
from tools.handle_input import handle_predict_input
from predict.classifiers import AhaabAtomClassifierPKD, AhaabAtomClassifierBinder

# pytorch imports
import torch
import torch.optim as optim

# numpy
import numpy as np

# python
from pathlib import Path

def predict_ahaab_atom(feature_data, feature_labels, weights, reference_features):
    '''
    Usage:
    $ predict_ahaab_atom(*args)

    Positional arguments:
    > feature_data: numpy array generated by validate_predict_input
                    containing features of models to predict.
    > feature_labels: list of labels for featurized data
    > weights:      pytorch file to load with classifier weights
    > reference_features: Reference AHAAB features file containing featurized
                          complexes, to use for standardizing data.
    
    Keyword arguments:
    None

    Outputs:
    > Prints results to stdout and produces a text file
    with model predictions.
    '''

    # Make sure weights and reference features exist:
    if not Path(weights).is_file():
        formats.error(f"Classifier weights file {weights} not found!")
        return
    elif not Path(reference_features).is_file():
        formats.error(f"Reference features for data standardization not found!")
        return

    ref_features=validate_predict_input(reference_features,test=False)
    ref_features=reference_features[0]

    model = AhaabAtomClassifier()
    model.load_state_dict(torch.load(weights))
    model.eval()

    # Standardize data and make predictions. Append predicted values to a list, and return labels and predicted values for downstream processing
    pkd=[]
    formats.notice("{:^20}{:^15}".format("Complex Name","Predicted pKd"))
    for i,l in enumerate(feature_labels):
        print(f"{l:<20}  Ac")
        X=standardize_feature_data(feature_data[i,:],scaleto=ref_features)
        if not X.any():
            formats.error(f"Training set {s} aborted")
            continue
        X=torch.tensor(X)

        with torch.no_grad():
            pred_pkd=model(X)
        print(f"{l:^20}{pred_pkd:^15}")
        pkd.append(pred_pkd)

    return