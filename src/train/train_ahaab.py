'''
AHAAB train_ahaab submodule
Part of the AHAAB train module

ahaab/
└──train
    └──train_ahaab.py

Submodule list:
    
    === train_ahaab_atom ===
'''
# ahaab
from tools import formats
from predict.classifiers import AhaabAtomClassifierPKD, AhaabAtomClassifierBinder

# pytorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# numpy
import numpy as np

# pandas
import pandas as pd

# python
import os
import sys

<<<<<<< HEAD
def train_ahaab_atom(**kwargs):
    '''
    Usage:
    $ train_ahaab_atom(**kwargs)

    Keyword arguments:
=======
def train_ahaab_atom(ahaab_dataset):
    '''
    Usage:
    $ train_ahaab_atom(*args)

    Positional arguments:
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
    > ahaab_dataset:    PyTorch dataset generated by
                        handle_training_input. Contains
                        columns with filenames for each
                        training/testing dataset.
<<<<<<< HEAD
    > num_features:     The number of features to pass
                        to the neural network.
=======
    
    Keyword arguments:
    None
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db

    Outputs:
    > Pytorch .pt files that contain trained model
    parameters, and a list of filenames for those
    model parameters.
    '''
<<<<<<< HEAD
    ahaab_dataset=kwargs["ahaab_dataset"]
    num_features=kwargs["num_features"]
# ========================================================
    def train_pytorch(ahaab_train_loader, **kwargs):
=======
# =============================================================================
    def train_pytorch(ahaab_train_loader, classifier="pkd"):
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
        '''
        Usage:
        $ train_pytorch(*args,**kwargs)

        Positional arguments:
        > ahaab_train_loader:   Pytorch dataloader containing
                                features and labels
        
        Keyword arguments:
        > classifier:   Value indicating which classifier - the
<<<<<<< HEAD
                        regression or binary classifier - should 
                        be trained
        > num_features: Number of features to use in the neural
                        network
=======
                        regression or binary - should be trained
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db

        Outputs:
        Tuple containing the following positional values:
        > Trained model parameters
        > Loss trajectory data
        '''
<<<<<<< HEAD
        classifier=kwargs["classifier"]
        num_features=kwargs["num_features"]
        # Define which classifier to use - regression (PKD) or classifier (Binder) and assign the corresponding loss function
        if classifier=="regression":
            model=AhaabAtomClassifierPKD(num_features)
            loss_fn=nn.MSELoss()
        elif classifier=="classifier":
            model=AhaabAtomClassifierBinder(num_features)
=======

        # Define which classifier to use - regression (PKD) or binary (Binder) and assign the corresponding loss function
        if classifier=="pkd":
            model=AhaabAtomClassifierPKD()
            loss_fn=nn.MSELoss()
        elif classifier=="bind":
            model=AhaabAtomClassifierBinder()
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
            loss_fn=nn.BCELoss()
        
        optimizer=optim.Adam(model.parameters(), lr=1e-4)
        n_epochs=100
        loss_trajectory=[]
        model.train()
        formats.notice("{:^5}     {:<15}".format("Epoch #","Loss"))
        for epoch in range(n_epochs):
            for X_batch,Y_batch in ahaab_train_loader:
                # Make a prediction
                y_pred=model(X_batch)
                # Compute the loss
                loss=loss_fn(y_pred,Y_batch)
                # Run the optimizer and update the weights
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            print(f"{epoch:^5}     {loss:.4e}")
            loss_trajectory.append(loss.detach().numpy())

        return model,loss_trajectory
<<<<<<< HEAD
# ========================================================
=======
# =============================================================================
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
    def test_pytorch(ahaab_test_loader, model):
        '''
        Usage:
        $ test_pytroch(*args)

        Positional arguments:
        > ahaab_test_loader:    Pytorch dataloader containing
                                features and labels
        > model:                Trained model from
                                train_ahaab.train_pytorch()

        Keyword arguments:
        None

        Outputs:
        Tuple containing the following positional values:
        > Model prediction for each pKd value in the test set
        > Actual pKd values
        > Absolute difference in magnitude between predicted
          and actual pKd
        '''
        model.eval()
        pKd_pred=[]
        pKd_act=[]
        pKd_err=[]
        with torch.no_grad():
            for X,Y in ahaab_test_loader:
                y_pred=model(X)
                err=np.abs(Y-y_pred)
                pKd_pred.append(y_pred.item())
                pKd_act.append(Y.item())
                pKd_err.append(err.item())
        return pKd_act,pKd_pred,pKd_err
<<<<<<< HEAD
# ========================================================
=======
# =============================================================================
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
    # Iterate through the list of tuples, loading each feature and label dataset using pytorch's DataLoader class. Then train the model at each iteration, saving the statistics of each run to a file for analysis.
    output_data=pd.DataFrame()
    for i,elem in enumerate(ahaab_dataset):
        formats.notice(f"Now training using dataset {i+1} of {len(ahaab_dataset)}")
        train_dataset=elem[0]
        train_idx=elem[1]
        test_dataset=elem[2]
        test_idx=elem[3]

        # For the first iteration, check if user passed binary binding data (to train the binding classifier) or pkd values (to train the pKd regression model). If all values in the first dataset are 0 or 1, assume the binding classifier is being trained.
        if i==0:
            classifier=""
            for x,y in train_dataset:
                if y!=0 and y!=1:
<<<<<<< HEAD
                    classifier="regression"
                    formats.notice("Dataset containing pKd values detected. Regression classifier will be trained.")
                    break
            if not classifier:
                classifier="classifier"
=======
                    classifier="pkd"
                    formats.notice("Dataset containing pKd values detected. Regression classifier will be trained.")
                    break
            if not classifier:
                classifier="bind"
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
                formats.notice("Dataset containing boolean values detected. Binding binary classifier will be trained.")

        # For each cross validation set, train the model:
        ahaab_train_loader=DataLoader(train_dataset,batch_size=10,shuffle=True)
<<<<<<< HEAD
        train_model_data=train_pytorch(
            ahaab_train_loader,
            classifier=classifier,
            num_features=num_features)
=======
        train_model_data=train_pytorch(ahaab_train_loader,classifier=classifier)
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
        model=train_model_data[0]
        output_data=pd.concat([output_data,pd.Series(train_model_data[1],name=f"Loss_set-{i}")],axis=1)

        # Test the model:
        formats.notice(f"Testing model trained using dataset {i}...")
        ahaab_test_loader=DataLoader(test_dataset,shuffle=False)
        test_model_data=test_pytorch(ahaab_test_loader,model)
        output_data=pd.concat([output_data,
                   pd.Series(test_model_data[0],name=f"pKd_set-{i}"),
                   pd.Series(test_model_data[1],name=f"pKd-pred_set-{i}"),
                   pd.Series(test_model_data[2],name=f"err_set-{i}")],axis=1)

        # Save the trained model:
<<<<<<< HEAD
        torch.save(model.state_dict(),f"ahaab_{classifier}_model_{i}.pth")
        formats.notice(f"Model state saved to ahaab_model_{i}.pth")

    output_fn=os.path.abspath(f"ahaab_{classifier}_training_summary.csv")
=======
        torch.save(model.state_dict(),f"ahaab_model_{i}.pth")
        formats.notice(f"Model state saved to ahaab_model_{i}.pth")

    output_fn=os.path.abspath("ahaab_training_summary.csv")
>>>>>>> 4aaaf06f474a91f00483a2a78237897414a871db
    formats.notice(f"Saving testing statistics and model performance to {output_fn}")
    output_data.to_csv(output_fn,index=False)

    return train_model_data,test_model_data