'''
AHAAB train_ahaab submodule
Part of the AHAAB train module

ahaab/
└──train
    └──train_ahaab.py

Submodule list:
    
    === train_ahaab_atom ===
'''
# AHAAB imports
from tools import formats
from tools.retrieve_data import standardize_feature_data
from predict.classifiers import AhaabAtomClassifier

# pytorch imports
import torch
import torch.optim as optim

# numpy
import numpy as np

# pandas
import pandas as pd

# python
import os
import sys

def train_ahaab_atom(train_test_files):
    '''
    Usage:
    $ train_ahaab_atom(*args)

    Positional arguments:
    > train_test_files: Pandas dataframe generated by
                        validate_training_input. Contains
                        columns with filenames for each
                        training/testing dataset. 
    
    Keyword arguments:
    None

    Outputs:
    > Pytorch .pt files that contain trained model
    parameters, and a list of filenames for those
    model parameters.
    '''

    def train_pytorch(X,y):
        '''
        Usage:
        $ train_pytorch(*args)

        Positional arguments:
        > X: Numpy matrix with featurized data
        > y: Numpy matrix with pKd values 
        
        Keyword arguments:
        None

        Outputs:
        > Trained model parameters
        > Loss trajectory data
        '''

        model=AhaabAtomClassifier()
        loss_fn=nn.MSELoss()
        optimizer=optim.Adam(model.parameters(), lr=0.001)
        n_epochs=100
        batch_size=10
        loss_trajectory=[]
        formats.notice("{:^5}     {:<5}".format("Epoch #","Loss"))
        for epoch in range(n_epochs):
            for i in range(0,len(X),batch_size):
                # For each batch in each epoch, make a prediction with the defined model:
                Xbatch=X[i:i+batch_size]
                y_pred=model(Xbatch)
                ybatch=y[i:i+batch_size]
                # Compute the loss
                loss=loss_fn(y_pred,ybatch)
                # Run the optimizer and update the weights
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                loss_trajectory.append(loss)
            print(f"{epoch:^5}     {loss:.2f}")

        return model,loss_trajectory

    # Load the data with numpy
    num_sets=len(train_test_files)
    model_state_filenames=[os.path.abspath("ahaab_pytorch_model_"+str(x)+".pt") for x in range(num_sets)]
    training_loss_filenames=[os.path.abspath("ahaab_pytorch_model_"+str(x)+"_loss.pt") for x in range(num_sets)]
    for s in range(0,num_sets):
        fx=os.path.abspath(train_test_files["features_train"].iloc[s])
        fy=os.path.abspath(train_test_files["pkd_train"].iloc[s])
        X=np.loadtxt(fx,delimiter=",")
        # Scale the training data to itself
        X=standardize_feature_data(X)
        if not X.any():
            formats.error(f"Training set {s} aborted")
            continue

        X=torch.tensor(X,dtype=torch.float32)
        y=np.loadtxt(fy,delimiter=",")
        y=torch.tensor(y,dtype=torch.float32)
        # Change the size of output to match that of input
        y=y.view(len(y),1)
        
        model_data=train_pytorch(X,y)
        model=model_data[0]
        loss_trajectory=model_data[1]
        torch.save(model.state_dict(),model_state_filenames[s])
        formats.notice(f"Model state saved for data set {s}")
        with open(training_loss_filenames[s], "w") as f:
            for l in loss_trajectory:
                f.write(f"{l}\n")
        formats.notice(f"Loss trajectory saved for data set {s}")

    return model_state_filenames