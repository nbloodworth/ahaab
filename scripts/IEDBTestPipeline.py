def help_msg(name=None):

    return '''
    ========================OVERVIEW=========================
    Utility for building input for FlexPepDock refinement:
    Will take a .fasta file with a list of epitopes and
    an HLA allele of interest and build input files for a
    FlexPepDock refinement run, by creating threaded MHC/
    peptide structures from a local database (created by
    calling MHCdatabase().build() from HLA_db.py). Produces
    an input file, options file, and slurm file for use
    on clusters that use SLURM for job scheduling. 

    Also used for querying IEDB database and building 
    structural models from resultant query:
    Takes in an HLA or MHC-I allele name (such as "A*02:01").
    Searches the IEDB for all epitopes with experimentally
    determined binding affinity data for that allele. 
    Sorts, removes duplicates, and returns (1)
    a fasta file without flagged values (those that passed
    the tests in the data validation stage) and (2) a 
    tab-delimited text file with epitope IDs, pubmed IDs,
    and quantitative binding values. These should be reviewed
    manually, and bad values deleted after literature review.
    The curated file can then be passed back to this utility
    with the original output fasta file, and the two will be
    combined before being shunted down the pipeline as
    described.

    ==========================PYTHON DEPENDENCIES==========================
    BioPython
    pandas
    numpy
    requests
    formats.py (custom class in same folder as IEDBTestPipeline.py)
    HLA_db.py (custom class in same folder as IEDBTestPipeline.py)
    
    ===========================PIPELINE SUMMARY============================

    |==>Retrieve epitope list from IEDB. [Optional step 1]
    |    
    |   python IEDBTestPipeline.py --IEDBquery A*02:01
    |
    |   >Epitopes processed: 4500000   Epitopes found: 13342     
    |   >Flagged epitope data located in flagged_HLA-A*02:01_IEDB_data.csv
    |   >Filtered IEDB data written to cleaned_HLA-A*02:01_IEDB_data.csv
    |   >Epitope sequences written to HLA-A*02:01.fasta
    |
    |==>Build fasta file from curated data. Include NCAA
    |   if applicable. NOTE: Can pass a custom .fasta file followed
    |   by a 0 if step 1 was omitted.
    |
    |   python IEDBTestPipeline.py --buildFasta HLA-A*02:01.fasta \\
    |                              flagged_HLA-A*02:01_IEDB_data.csv
    |
    |   OR
    |
    |   python IEDBTestPipeline.py --buildFasta custom.fasta 0
    |
    |   >Creates one folder for each sequence in the .fasta file, named
    |    after the sequence <record_name>
    |   >Folder contents:
    |     <record_name>_input.pdb
    |                          : Prepacked input PDB for FlexPepDock
    |                            Refinement
    |     <record_name>.fasta  : fasta file with peptide sequence,
    |                            modified to include NCAA if applicable
    |     <record_name>.slurm  : SLURM file with options specified by
    |                            user if --slurm argument passed
    |     <record_name>.options: .options file for Rosetta FlexPepDock
    |                            with additional values specified by
    |                            user if --options argument passed
    |     flexpepdock_input_<record_name>.xml
    |                          : RosettaScripts protocol for building
    |                            input for FlexPepDock Refinement
    |                            production run
    |     tmp_receptor.pdb     : Model of MHC-1/HLA receptor
    |                            (specified by user with --receptor
    |                            or generated by AlphaFold2)
    |     score.sc             : Scorefile for input PDB generated by
    |                            FlexPepDock prepack
    '''
# Dependencies from standard python librerary
import argparse 		
import sys
import os
import multiprocessing
from pathlib import Path
from zipfile import ZipFile
import shutil
import xml.etree.ElementTree as ET #import ElementTree to handle xml file creation
import subprocess

# Dependencies from other libraries
import requests
import pandas as pd

## Biopython
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from Bio import pairwise2
from Bio import AlignIO
from Bio.Align import substitution_matrices
from Bio import PDB
from Bio.PDB import PDBParser
from Bio.PDB import PDBIO

# Custom dependencies
from HLA_db import MHCdatabase

##==notify==##
class notify:
	# A class to format our warnings, errors, and notifications as they are printed to command line
	# To use: call constructor and pass optional arguments for warning color, error color, and notification color (defaults are
	# yellow, red, and blue). warn(), error(), and notice() methods will take in a string and format it appropriately,
	# printing it to the command line. Syntax: notify(warncolor='some color', errorcolor='some color', noticecolor='some color').
	# Uses ANSI formatting codes.
	
	# Define some default values that can be called from any instance for quick custom formatting
	RED='\033[91m'
	YELLOW='\033[93m'
	GREEN='\033[92m'
	BLUE='\033[94m'
	CYAN='\033[96'
	MAGENTA='\033[95m'
	BLACK='\033[90m'
	WHITE='\033[97m'
	END='\033[0m'
	BOLD='\033[1m'
	
	# Constructor - takes in a few optional arguments that allows you to change the default notification values based on
	# the class of notification (warning, error, or notice)
	def __init__(self,warncolor='YELLOW',errorcolor='RED',noticecolor='GREEN'):
		codes=['\033[91m','\033[93m','\033[92m','\033[94m','\033[95m']
		colors=['RED','YELLOW','GREEN','BLUE','PURPLE']
		colordict = dict(zip(colors,codes))
		# print(colordict)
		self.warn_color=colordict[warncolor]
		self.error_color=colordict[errorcolor]
		self.notice_color=colordict[noticecolor]
		if warncolor in colors:
			self.warn_color=colordict[warncolor]
		if errorcolor in colors:
			self.error_color=colordict[errorcolor]
		if noticecolor in colors:
			self.notice_color=colordict[noticecolor]

	# Notify functions that will automatically format and print a notification depending on its classification
	# Will optionally return the formatted message instead of printing if if passed toprint=False
	def warn(self,msg,toprint=True):
		fmsg='{}{}[WARNING]: {}{}'.format(self.warn_color,self.BOLD,msg,self.END)
		if toprint:
			print(fmsg)
		else:
			return fmsg

	def error(self,msg,toprint=True):
		fmsg='{}{}[ERROR]: {}{}'.format(self.error_color,self.BOLD,msg,self.END)
		if toprint:
			print(fmsg)
		else:
			return fmsg

	def notice(self,msg,toprint=True):
		fmsg='{}{}{}{}'.format(self.notice_color,self.BOLD,msg,self.END)
		if toprint:
			print(fmsg)
		else:
			return fmsg

##==clean_peplist==##

def clean_peplist(peplist,prefix=""):

    peplist.insert(0,"Flag",False) # Set a flag value here in case we need to do a manual review
    # Remove spaces from column names to make things a little easier
    peplist.columns=peplist.columns.str.replace(' ','_')
    peplist.columns=peplist.columns.str.replace('/','-')

    # Filter data: keep only relevant columns and only rows that contain binding affinity data
    # peplist=peplist[['Epitope_ID','PubMed_ID', 'Description', 'Parent_Protein', 'Parent_Protein_Accession', 'Method-Technique', 'Assay_Group', 'Quantitative_measurement', 'PDB_ID', 'Allele_Name']]

    # peplist=peplist.loc[(peplist['Assay_Group']=='dissociation constant KD (~IC50)') | (peplist['Assay_Group']=='half maximal inhibitory concentration (IC50)')]

    start_len=len(peplist)
    print('{} starting records'.format(start_len))
    enames = peplist.Epitope_ID.unique()
    print('{} unique epitope IDs'.format(len(enames)))

    print("Filtering duplicate epitopes...")
    filtered_count=0
    for e in enames:
        filtered_count+=1
        if filtered_count%10==0 or filtered_count==len(enames):
            print(f"Filtered {filtered_count:<10} of {len(enames):<10}",end="\r")
        tmp=peplist.loc[(peplist['Epitope_ID']==e)].copy()
        # First let's deal with case of two identical records (most common occurance)
        if len(tmp)==2:
        # First case: if only two records with only one having a pubmed ID, remove one without 'PubMed_ID'
            if pd.isna(tmp['PubMed_ID']).any() and not pd.isna(tmp['PubMed_ID']).all():
                peplist.drop(tmp.index[(pd.isna(tmp['PubMed_ID']))].tolist()[0], inplace=True)
        # Second case: if both have no pubmed ID and differ by more than 10nm, remove them both
            elif pd.isna(tmp['PubMed_ID']).all() and abs((tmp['Quantitative_measurement'].iloc[0]-tmp['Quantitative_measurement'].iloc[1]))>10:
                peplist.drop(tmp.index,inplace=True)
        # Third case: if both have pubmed IDs and do NOT differ by more than 10nm, remove the first (arbitrary)
            elif ~pd.isna(tmp['PubMed_ID']).all() and abs((tmp['Quantitative_measurement'].iloc[0]-tmp['Quantitative_measurement'].iloc[1]))<10:
                peplist.drop(tmp.index[0],inplace=True)
        # Forth case: if both have pubmedIDs and do not meet these criteria, flag them for review
            elif ~pd.isna(tmp['PubMed_ID']).all():
                peplist.at[tmp.index[0],'Flag']=True
                peplist.at[tmp.index[1],'Flag']=True
        # Fifth case: if both have no pubmed IDs and differ by less than 10nm, remove the first (arbitrary)
            else:
                peplist.drop(tmp.index[0],inplace=True)

        # More than 2 records:
        elif len(tmp)>2:
            # If at least one record has a pubmed ID, remove the ones without IDs
            if pd.isna(tmp['PubMed_ID']).any() and not pd.isna(tmp['PubMed_ID']).all():
                peplist.drop(tmp.index[(pd.isna(tmp['PubMed_ID']))].tolist(), inplace=True)
                tmp.drop(tmp.index[(pd.isna(tmp['PubMed_ID']))].tolist(), inplace=True)
                # Next check if the values remaining are all the same. If not, then flag the remaining records for review
                if len(tmp['Quantitative_measurement'].unique())>1:
                    for i in tmp.index.tolist():
                        peplist.at[i,'Flag']=True
                # If all the values are equal, drop everything but the first (arbitrary)
                else:
                    peplist.drop(tmp.index.tolist()[1::], inplace=True)	
            # If none of the records has a pubmed ID and they are all different, remove them
            elif pd.isna(tmp['PubMed_ID']).all(): 
                if len(tmp['Quantitative_measurement'].unique())>1:
                    peplist.drop(tmp.index.tolist(), inplace=True)
            # If none of the records has a pubmed ID and they are all the same, drop everything but the first 
                else:
                    peplist.drop(tmp.index.tolist()[1::], inplace=True)

    # Save filtered IEDB data for postprocessing later
    columns_tosave=peplist.columns.tolist()
    columns_tosave.remove("Flag")

    save_dir=os.getcwd()
    save_filename="IEDB_data.csv"
    print()
    print('{} records dropped'.format(start_len-len(peplist)))
    flagged_records=pd.DataFrame(columns=columns_tosave)
    if peplist["Flag"].any():
        flagged_records=peplist.loc[(peplist['Flag']==True)].sort_values(by='Epitope_ID')
        print('{} flagged records'.format(len(flagged_records)))
        peplist=peplist.drop(flagged_records.index.tolist())
        flagged_records.drop("Flag",axis=1,inplace=True)
        flagged_fn=os.path.join(save_dir,prefix+"flagged_"+save_filename)
        flagged_records.to_csv(flagged_fn, index=False)
        print(f'Flagged epitope data located in {flagged_fn}')

    print('{} records remaining'.format(len(peplist)))

    IEDB_fn=os.path.join(save_dir,prefix+"cleaned_"+save_filename)

    print(f"Filtered IEDB data written to {IEDB_fn}")
    peplist[columns_tosave].to_csv(IEDB_fn,index=False)

    # Build fasta
    fasta=peplist[['Epitope_ID','Description']]
    fasta_names = '>'+fasta['Epitope_ID'].astype(str)
    fasta_names=fasta_names.tolist()
    fasta_sequences=fasta['Description'].tolist()

    fasta_fn=prefix+'{}.fasta'.format("IEDB_data")
    output_fasta_fh=open(fasta_fn,'w')
    for n,s in zip(fasta_names,fasta_sequences):
        if '+' not in s: #One last filtering step to remove sequences with NCAAs
            output_fasta_fh.write(n+'\n')
            output_fasta_fh.write(s+'\n')
        else:
            print(f"Omitting epitope {n}, sequence {s}")

    output_fasta_fh.close()
    print(f'Epitope sequences written to {fasta_fn}')		

    return fasta_fn,peplist,flagged_records

##==get_peplist==##
def get_peplist(allele,assay_values):

    # First we need to get the list of HLA alleles we will be retrieving data for
    print("Retrieving data from IEDB for ",end="")
    notfound=[]
    if "all" in allele:
        print("all available alleles and ",end="")
        allele=["all"]
    else:
        # If allele is not set to all, retrieve the list of alleles
        allele_list=[]
        for a in allele:
            # Check if user passes a filename
            if "@" in a:
                fname=a[1:]
                if Path(os.path.abspath(fname)).is_file():
                    with open(fname,"r") as f:
                        allele_list.extend(f.read().splitlines())
                else:
                    notfound.append(fname)
            else:

                allele_list.append(a)
        # Error-check the allele list to ensure it is properly formatted (remove new line characters and HLA- prefixes, etc.)
        for i,a in enumerate(allele):
            if "HLA-" not in a:
                allele[i]="HLA-"+a
        print(f"{len(allele)} alleles and ",end="")

    assay_values=[ele.replace("_"," ") for ele in assay_values]
    print(f"the following assay values:\n{', '.join(assay_values)}")
    if len(notfound)>0:
        notify().warn("The following allele list files were passed by the user but not located:\n{}".format(', '.join(notfound)))

    # Make sure the local copy of the IEDB is present in <MHC database location>/build. If not, download it.
    dbloc=MHCdatabase().locate
    iedb_data_fn=os.path.abspath(os.path.join(dbloc,"build","mhc_ligand_full.csv"))
    IEDB_DATA_LOC="https://www.iedb.org/downloader.php?file_name=doc/mhc_ligand_full_single_file.zip"
    if not Path(iedb_data_fn).is_file():
        notify().warn(f"Local copy of IEDB not found. Attempting download...")
        try:
            success=True
            test_download=requests.get(IEDB_DATA_LOC)
            if test_download.status_code != 200:
                success=False
        except requests.exceptions.RequestException:
            success=False
        if not success:
            notify().error("Unable to obtain local copy of IEDB. Terminating execution.")
            return False
        else:
            iedb_data_zip_fn=os.path.join(os.path.dirname(iedb_data_fn),"IEDB_data.zip")
            iedb_data_zip=requests.get(IEDB_DATA_LOC, stream=True)
            with open(iedb_data_zip_fn, mode="wb") as f:
                for chunk in iedb_data_zip.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
            with ZipFile(iedb_data_zip_fn,"r") as zfile:
                zfile.extractall(path=os.path.dirname(iedb_data_zip_fn))
            os.remove(iedb_data_zip_fn)
            print(f"Local copy of IEDB downloaded to {iedb_data_fn}")
    
    cols=['Epitope IRI','PubMed ID', 'Description', 'Parent Protein', 'Parent Protein IRI', 'Method/Technique', 'Assay Group', 'Quantitative measurement', 'PDB ID', 'Allele Name', 'MHC allele class']

    iedb_data=pd.DataFrame(columns=cols)
    chunksread=0
    chunksz=250000
    with pd.read_csv(iedb_data_fn,header=1,usecols=cols,chunksize=chunksz,low_memory=False) as reader:
        for chunk in reader:
            # Keep only epitopes with:
            # The allele(s) of interest
            if allele[0] != "all":
                chunkdata=chunk[chunk["Allele Name"].isin(allele)]
                
            else: # Collect binding affinity data for all human type 1 HLA alleles (A,B,C) by default
                chunkdata_A=chunk[chunk["Allele Name"].str.contains("HLA-A*",regex=False)]
                chunkdata_B=chunk[chunk["Allele Name"].str.contains("HLA-B*",regex=False)]
                chunkdata_C=chunk[chunk["Allele Name"].str.contains("HLA-C*",regex=False)]
                chunkdata=pd.concat([chunkdata_A,chunkdata_B,chunkdata_C],ignore_index=True)
            # And quantitative binding metrics
            iedb_data=pd.concat([iedb_data,chunkdata[chunkdata["Assay Group"].isin(assay_values)]],ignore_index=True)
            # tmp1=chunkdata[chunkdata["Assay Group"].str.contains("half maximal inhibitory concentration (IC50)",regex=False)]
            # tmp2=chunkdata[chunkdata["Assay Group"].str.contains("dissociation constant KD (~IC50)",regex=False)]
            # iedb_data=pd.concat([iedb_data,tmp1,tmp2],ignore_index=True)
            chunksread+=1
            print(f"Epitopes processed: {chunksread*chunksz:<10}Epitopes found: {len(iedb_data):<10}",end="\r")
    
    print()
    if len(iedb_data)==0:
        notify().error(f"No epitopes with values for {assay_values} found in the IEDB!")
        return None

    # Clean the data a little to make it compatible with clean_peplist()
    iedb_data=iedb_data.astype({'Epitope IRI':str, 'Parent Protein IRI':str})
    iedb_data["Epitope ID"]=iedb_data["Epitope IRI"].str.split("/").apply(lambda x: x[-1])
    iedb_data["Parent Protein Accession"]=iedb_data['Parent Protein IRI'].str.split("/").apply(lambda x: x[-1])
    iedb_data.drop(columns=['Epitope IRI', 'Parent Protein IRI'],inplace=True)
    iedb_data.reset_index(inplace=True,drop=True)

    # For this next part we will create a separate IEDB and fasta files for each allele/epitope combination
    # We will also combine the cleaned, flagged, and fasta data into single combined files to be stored in the working directory
    allele_list=iedb_data["Allele Name"].unique().tolist()
    cwd=os.getcwd()
    epitope_count=0
    fasta_filenames=[]
    all_cleaned_data=pd.DataFrame()
    all_flagged_data=pd.DataFrame()
    for a in allele_list:
        print(f"Processing epitopes for allele {a}")
        newpath=os.path.abspath(os.path.join(cwd,a))
        Path(newpath).mkdir(parents=True,exist_ok=True)
        os.chdir(newpath)
        tmp_data=clean_peplist(iedb_data.loc[iedb_data["Allele Name"]==a],prefix=a+"_")
        fasta_filenames.append(tmp_data[0])
        all_cleaned_data=pd.concat([all_cleaned_data,tmp_data[1]])
        all_flagged_data=pd.concat([all_flagged_data,tmp_data[2]])
        with open(tmp_data[0],"r") as f:
            lines=f.readlines()
        epitope_count+=len(lines)
        os.chdir(cwd)

    print(f"{int(epitope_count/2)} total epitopes written to fasta files for downstream processing")
    cleaned_data_fn=os.path.abspath("cleaned_IEDB_data_all.csv")
    flagged_data_fn=os.path.abspath("flagged_IEDB_data_all.csv")
    all_cleaned_data.to_csv(cleaned_data_fn)
    print(f"All cleaned IEDB data written to {cleaned_data_fn}")
    all_flagged_data.to_csv(flagged_data_fn)
    print(f"All flagged IEDB data written to {flagged_data_fn}")

    return fasta_filenames

##==fasta_rebuild==##
def fasta_rebuild(fasta_fn, seq_toAdd):

    try:
        test=int(seq_toAdd)
    except:
        test=str(seq_toAdd).lower()
        if test=='false':
            notify().notice('Running pipeline with provided fasta file {}'.format(fasta_fn))
            return fasta_fn
    else:
        if int(seq_toAdd)==0:
            notify().notice('Running pipeline with provided fasta file {}'.format(fasta_fn))
            return fasta_fn
    
    if not os.path.isfile(os.path.abspath(fasta_fn)):
        sys.exit(notify().error('.fasta file {} does not exist!'.format(fasta_fn)))
    elif not os.path.isfile(seq_toAdd):
        sys.exit(notify().error('Curated sequences file {} does not exist!'.format(seq_toAdd)))

    seq_fh=open(seq_toAdd,'r')
    seq_data=seq_fh.readlines()[1::]
    seq_fh.close()

    fasta_fh=open(fasta_fn,'a')
    for i,l in enumerate(seq_data):
        tmp_data = l.split(',')
        if i==len(seq_data)-1:
            fasta_fh.write('>{}\n{}'.format(tmp_data[0],tmp_data[2]))
        else:
            fasta_fh.write('>{}\n{}\n'.format(tmp_data[0],tmp_data[2]))
    fasta_fh.close()

    print('Curated sequences added to {}'.format(fasta_fn))

    return fasta_fn

##==add_NCAA==##

def add_NCAA(fasta_fn,params,position=False):
    
    fasta_fh = open(fasta_fn,"r")
    fasta_data = fasta_fh.readlines()
    fasta_fh.close()

    if not os.path.isfile(params):
        sys.exit(notify().error("Params file {} does not exist!".format(params)))
    else:
        params_fh=open(params,"r")
        ncaa_code=params_fh.readline().strip()
        ncaa_code=ncaa_code[-3::]
        print("NCAA 3-letter code: {}".format(ncaa_code))

    set_pos=False
    if position:
        try:
            int(position)
        except:
            notify().warn("{} is not a valid integer position. Ignoring in favor of default NCAA placeholder X".format(position))
        else:
            position=int(position)
            set_pos=True
    
    fasta_fh=open(fasta_fn,"w")
    numlines=len(fasta_data)
    seqcount=0
    for i,l in enumerate(fasta_data):

        tmpl=list(l.strip())
        
        if ">" not in tmpl and set_pos: #Make sure we have a sequence
            if position<=len(tmpl):
                tmpl[position-1]="X[{}]".format(ncaa_code)
                tmpl="".join(tmpl)
                fasta_fh.write(tmpl)
                seqcount+=1
                if i+1!=numlines:
                    fasta_fh.write("\n")
            else:
                notify().warn("Position {} is out of range for sequence {} of length {}".format(position, "".join(tmpl), len(tmpl)))
                fasta_fh.write("".join(tmpl))
                if i+1!=numlines:
                    fasta_fh.write("\n")
        elif ">" not in tmpl and not set_pos and "X" in tmpl: #If no position to replace is set, replace default placeholder value ("X")
            tmpl="".join(tmpl)
            tmpl=tmpl.replace("X","X[{}]".format(ncaa_code))
            fasta_fh.write(tmpl)
            seqcount+=1
            if i+1!=numlines:
                    fasta_fh.write("\n")
        else:
            fasta_fh.write("".join(tmpl))
            if i+1!=numlines:
                    fasta_fh.write("\n")

    fasta_fh.close()
    notify().notice("NCAA '{}' added to {} sequences".format(ncaa_code,seqcount))

    return

##==make_batch==##

def make_batch(fasta_fn, size=50):

    # Open file, read in sequence data and names as a dict, then define batch size
    fh=open(fasta_fn)
    lines = fh.readlines()
    fh.close()

    fnames=lines[0::2]
    seqs=lines[1::2]

    # Iterate thorugh sequence data, creating a new file in a new batch folder every time the iterator exceeds the alloted batch size
    b_count=size # batch count iterator - keeps track of whether or not we need to make a new batch directory
    b_dir_count=1 # batch directory count - start with 1 and add 1 each time we exceed our batch size quota
    # Create our first directory and open our first fasta file
    batch_dir=os.path.join(os.path.dirname(fasta_fn),"batch{}".format(b_dir_count))
    os.mkdir(batch_dir)
    tmp_fn=os.path.abspath(os.path.join(batch_dir,"batch{}.fasta".format(b_dir_count)))
    tmp_fh=open(tmp_fn,"w")

    seqdata=zip(fnames,seqs)

    blist=[tmp_fn] #Keep track of our fastas

    for ind, n in enumerate(seqdata):
        if ind<=b_count:
            tmp_fh.write(n[0])
            if ind-1==len(fnames):
                tmp_fh.write(n[1].strip())
            else:
                tmp_fh.write(n[1])
            # If we are within our batch size quota, write the next line to our current fasta
        elif ind>b_count:
            # If we have exceeded our batch size quota, close the current fasta, open the new directory, and open a new fasta to write to
            tmp_fh.write(n[0])
            tmp_fh.write(n[1].strip())
            tmp_fh.close()
            b_dir_count+=1
            batch_dir=os.path.join(os.path.dirname(fasta_fn),"batch{}".format(b_dir_count))
            os.mkdir(batch_dir)
            tmp_fn=os.path.abspath(os.path.join(batch_dir,"batch{}.fasta".format(b_dir_count)))
            tmp_fh=open(tmp_fn,"w")
            b_count+=size
            blist.append(tmp_fn)
    tmp_fh.close() #close the last open file
    return blist

##========================== thread_template ==========================##

def thread_template(input_fasta, db, ROSETTA_PATH, params=False, user_defined_receptor=None, ignore_perfect_match=False, find_worst_template=False):
    # Subroutine that takes in an input peptide, path to database with aligned peptide/MHC complexes, and desired allele
    # Finds a matching peptide with the best sequence identity then threads the input peptide onto the corresponding model
     # Takes the threaded model and writes a rosetta_scripts.xml file to generate an input structure for flexpepdock refinement
     # Optionally takes in a params file (with the position denoted by "X[NCAA]" in the input_fasta file), and incorporates the NCAA into the final starting model by mutating the amino acid at the appropriate position.
     # Fasta files with NCAA should be formatted per Rosetta guidelines (i.e., NCAA should be designated "X[NCAA_name]")

     # Create the allele - we will need to make sure it has a receptor present in the database as well
     allele=db.allele
     if user_defined_receptor!=None and Path(user_defined_receptor).is_file():
         chk_success=db.set_allele(allele, build_receptor=False) 
     else:
         chk_success=db.set_allele(allele)# This will automatically build the receptor if it is not present and create a list of alleles in the structure database to use as peptide backbone templates. Building a receptor with AlphaFold2 could take upwards of ~1h
     if not chk_success:
         notify().error(f"Failed to set allele to {allele}")
         return False

     # Retrieve the peptide sequences in the structural database for either the allele or (if unavailable) its next nearest sequence neighbor
     struct_data_fn=os.path.join(db.locate,"database.info")
     if Path(struct_data_fn).is_file(): 
         struct_data=pd.read_csv(struct_data_fn)
     else:
         notify().error(f"Unable to locate {struct_data_fn}! Does it exist?")
         return False

     # Retrieve the default receptor location
     receptor_default=db.receptor

     # Import the fasta file for the sequence to thread
     input_record = SeqIO.read(input_fasta, "fasta")
     input_record_originalSeq=input_record.seq # Save the original sequence as we will be modifying the record sequence later.
     
     # Clean the sequence for alignment if it is formatted for Rosetta with a NCAA placeholder, and save the 3-letter code for the NCAA for later use
     if "X" in input_record.seq:
         if not params:
             notify().error("NCAA placeholder character 'X' found in {}, but no params file provided\033[0m".format(input_fasta))
             return False
         else:
             pfile=open(os.path.abspath(params),"r")
             NCAA_name=pfile.readline().strip()[-3::] #Assumes that the three letter code for the NCAA is identical to what is in the provided params file (it should be, but this might miss edge cases where it is not, which would also cause Rosetta to crash later in the pipeline)
             pfile.close()
             input_record.seq=Seq(str(input_record.seq).replace("["+NCAA_name+"]",""))

     # Retrieve the peptide backbone template:
     if ignore_perfect_match:
         to_omit=["self"]
     else:
         to_omit=[]
     if find_worst_template:
         template_id_method="worst"
     else:
         template_id_method="best"

     # Get the PDB to use as a template for peptide threading (based on sequence similarity)
     if "X" in input_record.seq:
         queryseq=str(input_record.seq)
     else:
         queryseq=str(input_record_originalSeq)
     
     best_pdb=db.get_peptide_template(queryseq, method=template_id_method, omit=to_omit)
     # Retrieve the sequence for the best template identified
     best_sequence_data=db.query_mhc_database(gettemplate=best_pdb, quiet=True)[0]
     best_sequence=best_sequence_data["Epitope_Description"]

     best_pdb_fn=os.path.abspath(os.path.join(db.locate,"templates",best_pdb+".pdb"))
     # Now we build an rosetta_scripts.xml file to do the following:
     # 	(1) Sequentially mutate residues using the simple threading mover to match our desired sequence
     # 	(2) If the template sequence is longer than our input sequence by N residues, delete the last N residues from the threaded model
     # 	(3) Add the template (or user defined) MHC receptor to the threaded peptide from the database of aligned structures
     #	(4) Call FlexPepDock prepack on the output structure in preparation for refinement run 

     parent_dir=os.path.abspath(os.path.dirname(input_fasta))
     XML_filename = "{}/flexpepdock_input_{}.xml".format(parent_dir, os.path.basename(input_fasta)[0:-6])
     args=["{}/source/bin/rosetta_scripts.linuxgccrelease".format(ROSETTA_PATH), "-s", best_pdb_fn, "-parser:protocol", XML_filename, "-database", "{}/database".format(ROSETTA_PATH), "-overwrite","-mute","all"]
     N=len(best_sequence)-len(input_record.seq) #for resolving length disparities between input sequence and selected template

     # Get the correct peptide chain from the file
     db_records=pd.read_csv(os.path.join(db.locate,"database.info"))
     pChain=db_records["Antigen_PDB_Chain(s)"].loc[db_records["PDB_ID"]==best_pdb].values[0]
     mhcChain=db_records["MHC_PDB_Chain1"].loc[db_records["PDB_ID"]==best_pdb].values[0]

     # STEP 1: SIMPLE THREADING MOVER
     print("Building .xml for input model generation...")
     root=ET.Element("ROSETTASCRIPTS")
     movers=ET.SubElement(root,"MOVERS")

     movers_threadModel=ET.SubElement(movers, "SimpleThreadingMover", name="threadModel", pack_neighbors="1", start_position="1{}".format(pChain), thread_sequence=input_record.seq, skip_unknown_mutant="1", pack_rounds="1")
     protocols=ET.SubElement(root,"PROTOCOLS")
     protocols_add_threadModel=ET.SubElement(protocols,"Add", mover="threadModel")

     #	Step 1b: Mutate NCAA if present
     if params:
         for i,AA in enumerate(input_record.seq):
             if AA=="X":
                 tmp_pos=i+1
                 protocols_add_mutateRes=ET.SubElement(protocols,"Add", mover="mutateRes_{}".format(tmp_pos))
                 movers_mutateRes=ET.SubElement(movers, "MutateResidue", name="mutateRes_{}".format(tmp_pos), target="{}{}".format(tmp_pos,pChain), new_res=NCAA_name)
         args.extend(["-extra_res_fa", os.path.abspath(params)])

     # STEP 2: DELETE OVERHANGING RESIDUES
     if N>0:
         protocols_add_deleteRes=ET.SubElement(protocols,"Add", mover="deleteRes")
         movers_deleteRes=ET.SubElement(movers, "DeleteRegionMover", name="deleteRes", start="{}{}".format(len(input_record.seq)+1,pChain), end="{}{}".format(len(best_sequence),pChain))

     # STEP 3: ADD TEMPLATE (OR USER DEFINED) MHC TO POSE

     # Determine if we should use a user-defined receptor or the default receptor for this allele, and make sure the file(s) exist
     if user_defined_receptor!=None:
         if Path(os.path.abspath(user_defined_receptor)).is_file():
             print(f"Adding user defined receptor: {os.path.basename(user_defined_receptor)}")
             replace_receptor_fn=os.path.abspath(user_defined_receptor)
         else:
             notify().warn(f"User defined MHC model {os.path.abspath(user_defined_receptor)} not located.")
             print(f"Adding default receptor for {db.allele}")
             replace_receptor_fn=receptor_default
     else:
         print(f"Adding default receptor {db.receptor} for {db.allele}")
         replace_receptor_fn=receptor_default

     # Read in the file and copy it to a temporary file - IMPORTANT - assumes that the default receptor will have a single chain named "A"
     pdb_parser=PDBParser(QUIET=True)
     new_receptor_fn = os.path.join(parent_dir,"tmp_receptor.pdb") #Create temporary receptor file to graft into pose
     new_receptor = pdb_parser.get_structure("defaultReceptor", replace_receptor_fn)
     io=PDBIO()
     
     for chain in new_receptor.get_chains():
         if chain.get_id()=="A":
             io.set_structure(chain)
             io.save(new_receptor_fn)

     print("Temporary file {} created for adding to pose".format(new_receptor_fn))
     # First make sure the template and template receptor have the same number of residues.
     structure=pdb_parser.get_structure("template",best_pdb_fn)
     model=structure[0]
     chain=model[mhcChain]
     template_rescount=0
     for residue in chain:
         template_rescount+=1

     structure=pdb_parser.get_structure("receptor",new_receptor_fn)
     model=structure[0]
     chain=model["A"]
     receptor_rescount=0
     for residue in chain:
         receptor_rescount+=1 	
     # Use delete region to remove excess residues from the template
     resdiff=template_rescount-receptor_rescount
     if resdiff>0:
         print(f"Deleting residual residues {receptor_rescount}-{template_rescount} from {best_pdb}")
         protocols_add_deleteRes2=ET.SubElement(protocols,"Add", mover="deleteRes_receptor")
         movers_deleteRes2=ET.SubElement(movers, "DeleteRegionMover", name="deleteRes_receptor", start="{}{}".format(receptor_rescount+1,mhcChain), end="{}{}".format(template_rescount,mhcChain),rechain="1")

     # Use addchain mover to add new receptor to pose
     # Determine chain to swap
     chaindict={"A":1,"B":2,"C":3,"D":4,"E":5,"F":6,"G":7,"H":8,"I":9,"J":10,"K":11,"L":12,"M":13,"N":14,"O":15,"P":16,"Q":17,"R":18,"S":19,"T":20,"U":21,"V":22,"W":23,"X":24,"Y":25,"Z":26}
     chain_toSwap=chaindict[db_records["MHC_PDB_Chain1"].loc[db_records["PDB_ID"]==best_pdb].values[0]]

     # Add the new receptor. This will also (conveniently) align the new receptor to the old receptor.
     protocols_add_addReceptor=ET.SubElement(protocols,"Add", mover="addReceptor")
     movers_addReceptor=ET.SubElement(movers, "AddChain", name="addReceptor", file_name="{}".format(new_receptor_fn), swap_chain_number=f"{chain_toSwap}")

     # STEP 4: RUN FLEXPEPDOCK PREPACK
     protocols_add_prepack=ET.SubElement(protocols,"Add", mover="prepack")
     movers_prepack=ET.SubElement(movers, "FlexPepDock", name="prepack", ppk_only="1")

     # Write the final xml file
     xmltree=ET.ElementTree(root)
     ET.indent(xmltree,space='    ')
     xmltree.write(XML_filename)

     # Run the script in Rosetta
     notify().notice("Now running Rosetta FlexPepDock prepack protocol...")
     p=subprocess.run(args)

     # Rename the output .pdb file
     output_fn_old = os.path.join(parent_dir, "{}_0001.pdb".format(best_pdb))
     output_fn_new = os.path.join(parent_dir, "{}_input.pdb".format(os.path.basename(input_fasta)[0:-6]))

     shutil.move(output_fn_old, output_fn_new)

     print("\n{} saved and ready for production run.".format(output_fn_new))

     return output_fn_new # return the output filename for use in thread_all

##========================== thread_all ==========================##

def thread_all(peptide_list, mhcdb, ROSETTA_PATH, user_defined_receptor=None, params=False, for_production=[False,False], ignore_perfect_match=False, find_worst_template=None):
    # Wrapper for thread_template. Will make a folder and produce a prepacked starting model for each sequence. Optionally 
    # writes a slurm file and flexpepdock options file from templates for each peptide so prepared.
    # Variables:
    # 	peptide_list: A fasta file or text file with a list of epitope sequence(s)
    #	allele: MHC allele to dock to
    #	ROSETTA_PATH: path to local installation of Rosetta
    #	user_defined_receptor: .pdb file to use as MHC receptor (optional)
    #	params: path to Rosetta .params file for any NCAA (optional)
    #	for_production: list with slurm and options variables and values to create .slurm and .options files for a flexpepdock refinement production run
    #	ignore_perfect_match: Optional variable to set in order to avoid using the input epitope as its own sequence, if it has a structure available in the template database

    parent_dir = os.path.dirname(peptide_list) # The directory where the fasta file is located
    allele=mhcdb.allele
    # Set default slurm options as a dict
    slurm_options= {"mail-user": "nathaniel.c.bloodworth.1@vumc.org", "mail-type": "ALL", "ntasks": "1", "time": "45:00:00", "mem": "1G", "output": "/scratch/bloodwn/%A-%a.log"}

    if for_production[0]: # Update slurm_options with user values, if applicable
        update_slurm_options=[]
        update_slurm_values=[]
        for s in for_production[0]:
            if "=" in s:
                tmpvals = s.split("=")
                update_slurm_options.append(tmpvals[0])
                update_slurm_values.append(tmpvals[1])
                if len(tmpvals)>2:
                    notify().warn("Provided slurm #SBATCH option '{}' has multiple values assigned. Only the first ({}) will be used\033[0m".format(tmpvals[0], tmpvals[1]))
            else:
                notify().warn("Provided slurm #SBATCH option '{}' has no assigned value and will be ignored\033[0m".format(s))
        
        sbatch_toupdate = dict(zip(update_slurm_options, update_slurm_values))
        slurm_options.update(sbatch_toupdate)
        print("New #SBATCH variable/option pairs to be written to file:")
        for key, value in slurm_options.items():
            print("{} = {}".format(key, value))

    options_list=[]
    if for_production[1]: # Parse updated command line arguments, if applicable
        print("\nSetting command line options:")
        options = for_production[1]
        for option in options:
            if "=" in option:
                tmpvals=option.split("=")
                tmpvals[0]="-{}".format(tmpvals[0])
                options_list.append(tmpvals[0])
                options_list.append(tmpvals[1::])
                print(" ".join(tmpvals))
            else:
                options_list.append("-{}".format(option))
                print("-{}".format(option))
    
    # Make sure dumb user doesn't try to override the input filename			
    if "-in:file:s" in options_list:
        bad_file_loc = options_list.index("-in:file:s")+1
        if isinstance(options_list[bad_file_loc], list):
            options_list=options_list[0:bad_file_loc]+options_list[bad_file_loc+1:-1]
        options_list.remove("-in:file:s")
        notify().warn("Flag -in:file:s and associated value(s) will be ignored")

    # Will need to add some code here for dealing with passing native structures 

    # Make sure the necessary options exist, and add them if they don't. These include -pep_refine, -nstruct, -ex1, -ex2aro
    if "-database" not in options_list:
        options_list.append("-database")
        options_list.append(["{}/database".format(ROSETTA_PATH)])
    if "-pep_refine" not in options_list:
        options_list.append("-pep_refine")
    if "-nstruct" not in options_list:
        options_list.append("-nstruct")
        options_list.append(["{}".format(250)]) #Default structures per run
    if "-ex1" not in options_list:
        options_list.append("-ex1")
    if "-ex2aro" not in options_list:
        options_list.append("-ex2aro")

    # First thing we do is open the fasta files or peptide squence files and determine the number of sequences present:
    peptide_list_fh = open(peptide_list, "r")
    records = peptide_list_fh.read().splitlines()
    record_sequences=[x for x in records if ">" not in x]
    record_names=[x[1:] for x in records if ">" in x] # This should only populate if a fasta file is passed
    peptide_list_fh.close()

    # Some more error checking for bad input:
    if len(record_names)>0 and len(record_names) != len(record_sequences):
        notify().warn("{} sequences but {} sequence IDs! --threadprep takes either a .fasta file with a list of sequence and sequence IDs, or a list of peptide sequences (one per line)".format(len(record_sequences), len(record_names)))
    
    # Now we assign sequence names if we got a peptide sequences list rather than a fasta file:
    if len(record_names)==0:
        print("List of peptide sequences detected. Parsing with arbitrary name designation...")
        count_records=0
        for r in records:
            count_records+=1
            record_names.append("P{}".format(count_records))

    # Obtain rotamers filename
    if params:
        params = os.path.abspath(params)
        params_fh = open(os.path.abspath(params))
        rotamers_fn = os.path.join(os.path.dirname(params), params_fh.readlines()[-1].split(" ")[1])
        params_fh.close()

    # Time to iterate through each sequence and produce the folder containing the prepacked, input file, slurm file, options file, and output folder.
    for sequence, name in zip(record_sequences, record_names):
        
        options_list_tmp = options_list[:]
        print("\nNow processing:")
        notify().notice(f"Name:\t\t{name}")
        notify().notice(f"Sequence:\t{sequence}")

        # Wait just one minute...did we forget our params file?
        has_NCAA=False
        if "X" in sequence:
            if not params:
                notify().warn("Sequence {} contains placeholder residue 'X' for a NCAA, but no params file passed. Skipping...\033[0m".format(name))
                continue
            
            if not Path(os.path.abspath(rotamers_fn)).is_file():
                notify().warn("Rotamers file {} associated with provided params file {} not located. Sequence {} will be skipped...\033[0m".format(os.path.abspath(rotamers_fn), os.path.abspath(params), name))
                continue

            else:
                options_list_tmp.append("-in:file:extra_res_fa")
                options_list_tmp.append(["{}".format(os.path.basename(params))])
                has_NCAA=True
        
        # Ok, phew, glad we have it. Now let's move on.

        # Now we create our folder, move into the folder, write our fasta file, and pass that info to thread_template
        # Here is an overview of the file structure for the production run:
        # <Allele Name> (parent directory)
        # 	<output> 	(silent files go here)
        # 	<batch 1
        # 		<peptide 1>
        # 			peptide1.slurm
        # 			peptide1.options
        # 			peptide1.fasta
        # 			peptide1_input.pdb
        # 		<peptide 2>
        # 			...
        # 	...
        # 	<batch N>
        new_dir=os.path.join(parent_dir, "{}".format(name))
        os.mkdir(new_dir)
        os.chdir(new_dir)
        
        # Write our temporary fasta file
        tmp_fasta="{}.fasta".format(name)
        SeqIO.write(SeqRecord(Seq(sequence),id="{}".format(name)), tmp_fasta, "fasta")

        # Call thread_template
        if has_NCAA:
            input_filename=thread_template(tmp_fasta, mhcdb, ROSETTA_PATH, user_defined_receptor=user_defined_receptor, params=params, ignore_perfect_match=ignore_perfect_match,find_worst_template=find_worst_template)
        else:
            input_filename=thread_template(tmp_fasta, mhcdb, ROSETTA_PATH, user_defined_receptor=user_defined_receptor,ignore_perfect_match=ignore_perfect_match,find_worst_template=find_worst_template)
        
        if not input_filename:
            notify().error(f"Aborting {name}:\t{sequence}")
            os.chdir(parent_dir)
            continue

        # Make our slurm and options files
        slurm_fh = open("{}.slurm".format(name),"w")
        slurm_fh.write("#!/bin/bash\n")
        for key, value in slurm_options.items():
            slurm_fh.write("#SBATCH --{}={}\n".format(key,value))

        # allele_name=allele[0]+allele[2:4]+allele[5:]
        allele_name=allele
        # 
        # if has_NCAA:
        # 	K_KIS="KIS"
        # else:
        # 	K_KIS="K"
        slurm_fh.write("\nmodule load GCC/6.4.0-2.28\n")
        slurm_fh.write(f"{ROSETTA_PATH}/source/bin/FlexPepDocking.linuxgccrelease @{name}/{name}.options -out:path:all /home/bloodwn/Marcos/control/output -out:file:silent {name}.silent")
        slurm_fh.close()

        # Add path for out input threaded model
        options_list_tmp.append("-in:file:s")
        options_list_tmp.append(["{}/{}".format(name,os.path.basename(input_filename))])

        # options_list_tmp.append("-native")
        # options_list_tmp.append([f"/home/bloodwn/RefinementBenchmark/native/{name}.pdb"])
        
        # Iterate through options for our options file and write them in 
        options_fh = open("{}.options".format(name),"w")
        for x in options_list_tmp:
            if isinstance(x, list):
                for y in x:
                    options_fh.write(" {}".format(y))
                options_fh.write("\n")
            else:
                options_fh.write("\n{}".format(x))

        options_fh.close()

        # Move our params file and associated rotamers file into the folder if it exists
        if params:
            shutil.copyfile(os.path.abspath(params), os.path.join(new_dir, os.path.basename(params)))
            shutil.copyfile(rotamers_fn, os.path.join(new_dir, os.path.basename(rotamers_fn)))

        # Finally we move up one level to our original directory and repeat the process for every record
        os.chdir(parent_dir)

    return

##==MAIN BODY OF EXECUTION==##

# Parse input arguments
parser = argparse.ArgumentParser(description='IEDBTestPipeline.py', usage=help_msg())
# Arguments to initiate pipeline:
parser.add_argument('--IEDBquery', nargs='*', default=['all'], help='Initiates pipeline by querying a local copy of the IEDB database, extracting all epitopes belonging to a specified allele and returning a curated list for manual review (unless --skipreview is passed). Followed by one or more alleles in the format "A*01:01", a filename with one allele per line (preceded by "@", for example @hla_list.txt"), or "all" to get affinity binding data for all alleles in the database. Additional arguments include: "skip", to skip the query and proceed with processing with a fasta file; "ncaa" to include epitopes with NCAA in their sequences')
parser.add_argument('--assay',nargs='*',default=['qualitative_binding', 'dissociation_constant_KD_(~EC50)', 'ligand_presentation', 'half_life', '3D_structure', 'half_maximal_inhibitory_concentration_(IC50)', 'dissociation_constant_KD_(~IC50)', '50%_dissociation_temperature', 'dissociation_constant_KD', 'half_maximal_effective_concentration_(EC50)', 'off_rate'], help='Optional argument to specify assay values to retrieve. Can pass "help" to list all assay values available. IMPORTANT: list values with "_" character in place of spaces. For example: dissociation_constant_KD_(~IC50)')

# Alternative argument to initiate pipeline with only a fasta file, or a fasta file+curated sequence file generated by --IEDBquery:
parser.add_argument('--buildFasta', nargs=2, help='Followed by path to .fasta and curated sequences in .csv file. May pass False or 0 for second positional argument if you just want to run the pipeline with a pre-made .fasta file')

# Mandatory arguments for pipeline execution:
parser.add_argument('--setAllele', nargs=1, help='Specify the MHC allele that will serve as the receptor')
parser.add_argument('--batchSize', nargs=1, type=int, help='Number of sequences per batch for production run. If a batch size is small, then multiprocessing will attempt to produce input files using a larger number of CPUs simultaneously. Smallest value is 1.')

# Optional arguments for pipeline execution:
parser.add_argument('--skipReview', action='store_true', help='Designate that flagged data should be disregarded. Use in conjunction with other flags to proceed down the pipeline without interruption')
parser.add_argument('--params', nargs='*', help='Followed by path to params file and optional position to make an NCAA substitution')
parser.add_argument('--rosetta', nargs=1, default=["/dors/meilerlab/apps/rosetta/rosetta-3.13/main"], help='Optional flag for defining path to rosetta/main. Default is /dors/meilerlab/apps/rosetta/rosetta-3.13/main')
parser.add_argument('--mhcdatabase', nargs=1, help='Followed by a location to the MHC database (default is /home/<user>/Data/MHC_database')
parser.add_argument('--receptor', nargs=1, default=[None], help='Optional flag to define a receptor for --threadprep')
parser.add_argument('--slurm', nargs='*', help='Optional flag to make slurm file for flexpepdock refinment production run. Takes in an arbitrary number of arguments that correspond to #SBATCH option and value pairs, delimited as option=value')
parser.add_argument('--options', nargs='*', help='Optional flag to make options file for flexpepdock refinement production run. Takes in an arbitrary number of arguments that correspond to command-line options for flexpepdock refinement. Command line arguments with assigned values are designated with a "=" character. For example: input_file=my.file is translated to -input_file my.file. Multiple values for a single argument can be set with values separated by subsequent "=" characters. For example: set_values=val1=val2=val3 is parsed to -set_values val1 val2 val3')
parser.add_argument('--ignore_epitope_match', action="store_true",help="Optional flag to ignore sequence matches when identifying a template")
parser.add_argument('--find_worst_template',action="store_true",help="Optional flag to locate the worst template by sequence (rather than best)")

args=parser.parse_args()
if args.mhcdatabase:
    mhcdb=MHCdatabase(location=args.mhcdatabase[0])
else:
    mhcdb=MHCdatabase()

# Initiate pipeline with IEDB query file cleaning, fasta rebuilding, or premade fasta
if "skip" not in args.IEDBquery:
    if args.assay[0]=='help':
        sys.exit("===IEDB MHC binding assay values available:===\n{}".format('\n'.join(parser.get_default('assay'))))
    else:
        # Brief error check to make sure all requested assay values are valid:
        test=list(set(args.assay)-set(parser.get_default('assay')))
        if len(test)>0:
            notify().warn(f"User requested assay values {', '.join(test)} not available and will be ignored")
            print("===IEDB MHC binding assay values available:===\n{}".format('\n'.join(parser.get_default('assay'))))
        fasta_fn=get_peplist(args.IEDBquery,assay_values=args.assay)

    if fasta_fn==None:
        sys.exit(notify().error("Terminating pipeline"))
    if not args.skipReview:
        sys.exit(notify().notice('Exiting for manual review. Pass --buildFasta with .fasta file and curated sequences to resume'))
elif args.buildFasta:
    fasta_fn=fasta_rebuild(args.buildFasta[0],args.buildFasta[1])
else:
    sys.exit(notify().error('Must supply IEDB database query file with --IEDBquery or pass the --buildFasta after manual sequence curation to proceed with pipeline'))

# Make sure the user specified an allele that exists
if args.setAllele:
    chk = mhcdb.set_allele(args.setAllele[0])
    if not chk:
        sys.exit()
    print(f"MHC allele set to {mhcdb.allele}")
else:
    sys.exit(notify().error("Must supply valid allele with --setAllele flag!"))

# Add NCAA if --params is passed (with optional position to replace in sequence, indexed from 1)
add_params=False
if args.params:
    if len(args.params)>1:
        add_NCAA(fasta_fn, args.params[0], position=args.params[1])
    else:
        add_NCAA(fasta_fn, args.params[0])
    add_params=args.params[0]

# Next part of the pipeline: batch the fasta file
if args.batchSize:
    batchlist=make_batch(fasta_fn, args.batchSize[0])
else:
    batchlist=make_batch(fasta_fn)

# Now we thread. We will call the multiprocesser to run the threading subroutines on multiple cores at once, adding each job to the pool as we iterate through the batch folders we created.

# Call multiprocess (max CPU cores is available minus one)
pool=multiprocessing.Pool(multiprocessing.cpu_count()-1)
for d in batchlist:
    print(f"Threading peptides in {os.path.basename(d)}")
    # thread_all(d,mhcdb,args.rosetta[0],args.receptor[0],params=add_params, for_production=[args.slurm, args.options], ignore_perfect_match=args.ignore_epitope_match,find_worst_template=args.find_worst_template)
    pool.apply_async(thread_all, (d,mhcdb,args.rosetta[0],args.receptor[0]),dict(params=add_params, for_production=[args.slurm, args.options], ignore_perfect_match=args.ignore_epitope_match,find_worst_template=args.find_worst_template))
pool.close()
pool.join()

# os.system(f"mv batch*/* .")
# os.system(f"rm -r batch*")

notify().notice("Preprocessing complete. Files ready for transfer to cluster")